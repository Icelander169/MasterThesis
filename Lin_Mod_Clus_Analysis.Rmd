---
title: Linear model and cluster analysis of Influence factors and Public Transport
  subscriptions
author: "Gabriel Peier"
date: '2022-09-30'
output:
  pdf_document:
    number_sections: TRUE
    keep_tex: TRUE    
  html_document: default
---

```{r setup, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE)
```

# INTRODUCTION

In this script, the methodical part of the Linear Model and the Cluster Analysis is described and executed.

## Packages
For this analysis, several packages for the coding were needed. To make the use and reproducability easier, I list them here at the beginning of the paper. I did use the following packages:

```{r results="hide"}
library(tidyverse)             # ggplot2, dplyr, tidyr, readr, tibble
library(stringr)
library(dplyr)
library(caret)                 # data splitting and pre-processing
library(PerformanceAnalytics)  # special graphical comparisons of variables
library(ggpubr)                # for using ggbarplot
library(regclass)              # For VIF function (Variance Inflation Factor)
library(MASS)
```




## Loading data
```{r}
getwd()
d.inf_fac <- read.csv(("../Data/Cleaned/inf_fac_share.csv"))
d.inf_cnt <- read.csv(("../Data/Cleaned/inf_fac_count.csv")) 
```


## NA handling
In the dataset exists NaN values, which can not be handled within a lm function (in contrast to NA value). So the values have to be replaced:
```{r}
apply(is.na(d.inf_fac), 2, sum)
d.inf_fac[is.na(d.inf_fac) | d.inf_fac == "Inf"] <- NA
d.inf_cnt[is.na(d.inf_cnt) | d.inf_cnt == "Inf"] <- NA

## Removing all observations with NA!

d.inf_fac <- na.omit(d.inf_fac)
d.inf_cnt <- na.omit(d.inf_cnt)
```




## Graphical Analysis
In this chapter we will perform several analysis steps based on graphics to get a meaningful insight into the data. The used methods only cover a little part of the huge possibilities when it comes to visualizations. We mainly focused on the "ggplot"-package as it is mainly new for us and we wanted to go in deeper there as it comes with a wide range of options.

```{r fig.height=3, warning=FALSE}

# summary(d.inf_fac)

# glimpse(d.inf_fac)

d.inf_fac[,2:14] %>%
  keep(is.numeric) %>%
  gather() %>%
  ggplot(aes(value))  +                   # Plot the values
    facet_wrap(~ key, scales = "free") +  # In separate panels
    geom_density() +                      # show density lines
    theme(axis.text=element_text(size=6, face="bold")) # change axis size

d.inf_fac[,15:26] %>%
  keep(is.numeric) %>%
  gather() %>%
  ggplot(aes(value))  +                   # Plot the values
    facet_wrap(~ key, scales = "free") +  # In separate panels
    geom_density() +                      # show density lines
    theme(axis.text=element_text(size=6, face="bold")) # change axis size

d.inf_fac[,27:38] %>%
  keep(is.numeric) %>%
  gather() %>%
  ggplot(aes(value))  +                   # Plot the values
    facet_wrap(~ key, scales = "free") +  # In separate panels
    geom_density() +                      # show density lines
    theme(axis.text=element_text(size=6, face="bold")) # change axis size

d.inf_fac[,39:50] %>%
  keep(is.numeric) %>%
  gather() %>%
  ggplot(aes(value))  +                   # Plot the values
    facet_wrap(~ key, scales = "free") +  # In separate panels
    geom_density() +                      # show density lines
    theme(axis.text=element_text(size=6, face="bold")) # change axis size
```

When looking at the graphs, it quickly becomes apparent that some variables should be normalised so that they do not have too much weight. 
Thus, the data set is created as a normalized copy afterwards:


## Normalizing variables

The normalization is important due to some outstanding variables. However, this mainly affects those variables that have no shares, as these already lie between 0 and 1 anyway. But to standardize all continuous variables at the same scale, I will include all parameters except the target variable into the normalization:.
A normal scale function should do the necessary normalization in these cases.

```{r}
# NORMALIZING SHARE VALUES
d.norm <- d.inf_fac[,4:50]
print(colnames(d.norm))
# define columns to log-transform:
columns <- c("single_share", "married_share",
             "widowed_share", "divorced_share", "GA_share", "HTA_share",
             "FNT_share", "age0_20", "age20_40", "age40_60", "age60.", "birth_munic",
             "birth_cant", "birth_CH", "birth_notCH", "male", "female", 
             "resid_0_1y", "resid_1_5y", "resid_6_10y", "resid_10.y",
             "hh_1", "hh_2", "hh_3_5", "hh_6.",
             "PT_dist_medium", "PT_time_medium", "PT_dist_big",
             "PT_time_big", "str_dist_medium", "str_time_medium", 
             "str_dist_big", "str_time_big", "PT_fact_big", "PT_fact_medium",
             "bus_stops_per_pop", "train_stops_per_pop", "other_stops_per_pop",
             "bus_stat_per_1000", "train_stat_per_1000", "other_stat_per_1000",
             "comb_car_1000", "el_car_1000", "inbound_share", "outbound_share")

d.norm[columns] <- scale(d.norm[columns])
# summary(d.inf_fac)

# NORMALIZING COUNT DATA

# The count data has to be log-transformed, this should be enough.
d.norm.cnt <- d.inf_cnt[,4:49] 
# delete the GA_share column:
columns_cnt <- c(2:9, 11:46)
d.norm.cnt <- d.norm.cnt[columns_cnt]
print(colnames(d.norm.cnt))

d.norm.cnt[columns_cnt] <- scale(d.norm.cnt[columns_cnt])
```


## Correlation charts to factor groups

### Marital state
```{r fig.height=3.5}
chart.Correlation(d.inf_fac[,c(10:12, 6:9)], # +1 due to 0-values (log(0) = -Inf)
                  histogram=TRUE) # adding histograms to the plot
```

### Age segment
```{r fig.height=3.5}
chart.Correlation(d.inf_fac[,c(10:12, 13:16)], # +1 due to 0-values (log(0) = -Inf)
                  histogram=TRUE) # adding histograms to the plot
```

### Birth origin
```{r fig.height=3.5}
chart.Correlation(d.inf_fac[,c(10:12, 17:20)], # +1 due to 0-values (log(0) = -Inf)
                  histogram=TRUE) # adding histograms to the plot
```

### Gender
```{r fig.height=3.5}
chart.Correlation(d.inf_fac[,c(10:12, 21:22)], # +1 due to 0-values (log(0) = -Inf)
                  histogram=TRUE) # adding histograms to the plot
```

### Residence time
```{r fig.height=3.5}
chart.Correlation(d.inf_fac[,c(10:12, 23:26)], # +1 due to 0-values (log(0) = -Inf)
                  histogram=TRUE) # adding histograms to the plot
```

### Household size
```{r fig.height=3.5}
chart.Correlation(d.inf_fac[,c(10:12, 27:30)], # +1 due to 0-values (log(0) = -Inf)
                  histogram=TRUE) # adding histograms to the plot
```

### PT Distance and time to middle/big cities
```{r fig.height=3.5}
chart.Correlation(d.inf_fac[,c(10:12, 31:34)], # +1 due to 0-values (log(0) = -Inf)
                  histogram=TRUE) # adding histograms to the plot
```
### PT Factor compared to Street (Time consumption)
```{r fig.height=3.5}
chart.Correlation(d.inf_fac[,c(10:12, 39:40)], # +1 due to 0-values (log(0) = -Inf)
                  histogram=TRUE) # adding histograms to the plot
```

### Bus / train / other stops per population
```{r fig.height=3.5}
chart.Correlation(log(d.inf_fac[,c(10:12, 5, 41:45)]+1), # +1 due to 0-values (log(0) = -Inf)
                  histogram=TRUE) # adding histograms to the plot
```

### Car distribution
```{r fig.height=3.5}
chart.Correlation(d.inf_fac[,c(10:12, 47:48)], # +1 due to 0-values (log(0) = -Inf)
                  histogram=TRUE) # adding histograms to the plot
```

### Commuter statistics
```{r fig.height=3.5}
chart.Correlation(d.inf_fac[,c(10:12, 49:50)], # +1 due to 0-values (log(0) = -Inf)
                  histogram=TRUE) # adding histograms to the plot
```




## GA, HTA & FNT shares compared to languages:

```{r, warning=FALSE}
plot_HTA <- ggbarplot(d.inf_fac, x="language", y="HTA_share", 
                      fill="language", 
              #        facet.by="language", 
                      add = "mean_se",
                      xlab = "language",
                      legend = "right",
                      title = "HTA",
                      ggtheme = theme_cleveland()) + # better looking theme
  theme(axis.text=element_text(size=6.5, face="bold"))

plot_GA <- ggbarplot(d.inf_fac, x="language", y="GA_share", 
                      fill="language", 
                #       facet.by="language", 
                      add = "mean_se",
                      xlab = "language",
                      legend = "right",
                      title = "GA",
                      ggtheme = theme_cleveland()) +
  theme(axis.text=element_text(size=6.5, face="bold"))

plot_FNT <- ggbarplot(d.inf_fac, x="language", y="FNT_share", 
                      fill="language", 
                    #  facet.by="language", 
                      add = "mean_se",
                      xlab = "language",
                      legend = "right",
                      title = "FNT",
                      ggtheme = theme_cleveland()) +
  theme(axis.text=element_text(size=6.5, face="bold"))


share.comparison <- ggarrange(plot_GA, plot_HTA, plot_FNT, ncol=3) # two plots beside each other

annotate_figure(share.comparison, 
                top = text_grob("Comparison of mean share of GA & 
                                HTA sold in % of population", 
                                face = "bold")) # set over-all title ("top")
```

\newpage


# MODELLING SHARES
I will achieve the first objective of modelling the share of season tickets through two different approaches:

## Train / Test splitting Share/count datasets (original + normalized each)

### original share dataset
```{r}
set.seed(234) # reproducible
indexes <- createDataPartition(d.inf_fac$BFS_Nr, p = .8, list = FALSE) 
d.train <- d.inf_fac[indexes, ]
d.test <- d.inf_fac[-indexes, ]

sum(length(d.train$BFS_Nr) + length(d.test$BFS_Nr)) # control the split
length(d.inf_fac$BFS_Nr)
# print(d.train[1:5,])
```
### original count dataset
```{r}
set.seed(234) # reproducible
indexes <- createDataPartition(d.inf_cnt$BFS_Nr, p = .8, list = FALSE) 
d.cnt.train <- d.inf_cnt[indexes, ]
d.cnt.test <- d.inf_cnt[-indexes, ]

sum(length(d.cnt.train$BFS_Nr) + length(d.cnt.test$BFS_Nr)) # control the split
length(d.inf_cnt$BFS_Nr)
# print(d.train[1:5,])
```

### Normalized share dataset
```{r}
set.seed(234) # reproducible
indexes <- createDataPartition(d.inf_fac$BFS_Nr, p = .8, list = FALSE) 
d.norm.train <- d.norm[indexes, ]
d.norm.test <- d.norm[-indexes, ]
```

### Normalized count dataset
```{r}
set.seed(234) # reproducible
indexes <- createDataPartition(d.inf_cnt$BFS_Nr, p = .8, list = FALSE) 
d.norm.cnt.train <- d.norm.cnt[indexes, ]
d.norm.cnt.test <- d.norm.cnt[-indexes, ]
```



## Function for Model Evaluation
For the evaluation of our models we will use the following function, which displays the RMSE and the R_squared for our predicted values. 
```{r}
eval_results <- function(true, predicted) {
  SSE <- sum((predicted - true)^2, na.rm = TRUE)
  SST <- sum((true - mean(true, na.rm = TRUE))^2, na.rm = TRUE)
  R_square <- 1 - SSE / SST
  RMSE = sqrt(SSE/length(true))

# Model performance metrics
data.frame(RMSE = RMSE,
           Rsquare = R_square)
}
```



## Parameter selection


## Linear model of GA_count

First, I try to build a linear model, which models the absolute numbers of GA's per municipality. 
Due to the amount of count data
### Building linear model with all variables of original dataset
```{r}
lm0 <- lm(GA_share ~ . -HTA_share - FNT_share, data = d.train[, 4:50]) # Model with all predictors
summary(lm0) ## We see that there are 4 parameters with NA value => check with alias function
alias(lm0) # High collinearities between the variables (makes sense!) => Remove these 4 variables from model
lm01 <- lm(GA_share ~ . - HTA_share - FNT_share - 
                   X.60 - birth_notCH - female - hh_.6, data = d.train[, 4:50])
summary(lm01) # looks good this time!

MASS::stepAIC(lm01, direction = "both", trace = FALSE)
```
The stepAIC function gives me now a suggested formula, what can be used for a second try. The coefficients show the weights.


## Generalized Linear model with family "Binomial"
With a linear model, no probability distributions can be predicted, as values above 1 and below 0 are possible, resulting in meaningless values concerning the share. Therefore, a second, logistic approach comes into play, which strictly forecasts values between 0 and 1. Although I do not have values for individuals with regard to the share and thus do not model a binary target variable, a multinomial logit model approach is described for aggregated data using group variable, which is in my case the municipality (Morais et al., 2016).
Quasibinomial glm fulfills the criteria here:

### GLM binomial with 



### GLM binomial with ORIGINAL data of SHARE dataset

```{r}
### Modelling GLM with share data:

## Model set up
glmodel.0 <- glm(GA_share ~ . - HTA_share - FNT_share - 
                   age60. - birth_notCH - female - hh_6., 
                family = binomial, data=d.inf_fac[,4:50], weights=d.inf_fac$pop_count_BFS) 
# for binomial model, input must be given as one of these 3 categories: 0 or 1, c(success, total), or share with corresponding weights!
summary(glmodel.0)


# Forward and backwards stepwise!
glmodel.AIC <- stepAIC(glmodel.0, direction="both", trace=FALSE)
glmodel.AIC$formula
glmodel.AIC$anova
summary(glmodel.AIC)


# evaluate graphically
glmteAIC <- ggplot(d.test, aes(y=GA_share ,x=predict(glmodel.AIC, d.test, type="response"))) +
  labs(y= "actual", x = "predicted") + ggtitle("TEST: actual vs predicted") + 
  geom_point(shape = 1, alpha = 0.7)

glmtrAIC <- ggplot(d.train,aes(y=GA_share, x=predict(glmodel.AIC, d.train, type="response"))) +
  labs(y= "actual", x = "predicted") + ggtitle("TRAIN: actual vs predicted") + 
  geom_point(shape = 1, alpha = 0.7)

gridExtra::grid.arrange(nrow = 1, ncol = 2, 
                        glmteAIC + stat_smooth(size=0.5, method="glm",se=FALSE),
                        glmtrAIC + stat_smooth(size=0.5, method="glm",se=FALSE),
                        top="GLM binomial with ORIGINAL data of SHARE dataset, Best Fit with AIC")

## Evaluating R Square
glmodel.AIC.eval.train <- eval_results(d.train$GA_share, predict(glmodel.AIC, d.train, type="response")) 
glmodel.AIC.eval.test <- eval_results(d.test$GA_share, predict(glmodel.AIC, d.test, type="response"))

writeLines(paste("GLM.AIC: RMSE train: ", round(glmodel.AIC.eval.train$RMSE, 5), 
                 "\nGLM.AIC: Rsquare train: ",round(glmodel.AIC.eval.train$Rsquare, 3),
                 "\nGLM.AIC: RMSE test: ", round(glmodel.AIC.eval.test$RMSE, 5), 
                 "\nGLM.AIC: Rsquare test: ", round(glmodel.AIC.eval.test$Rsquare, 3)))

```

### GLM binomial with NORMALIZED data of SHARE dataset

```{r}
### Modelling GLM with share data:

## Model set up
glmodel.norm.0 <- glm(GA_share ~ . - HTA_share - FNT_share - 
                   age60. - birth_notCH - female - hh_6., 
                family = binomial, data=d.norm, weights=d.norm$pop_count_BFS) 
# for binomial model, input must be given as one of these 3 categories: 0 or 1, c(success, total), or share with corresponding weights!
summary(glmodel.norm.0)


# Forward and backwards stepwise!
glmodel.norm.AIC <- stepAIC(glmodel.norm.0, direction="both", trace=FALSE)
glmodel.norm.AIC$formula
glmodel.norm.AIC$anova
summary(glmodel.norm.AIC)


# evaluate graphically
glmteAIC.norm <- ggplot(d.norm.test, aes(y=GA_share ,x=predict(glmodel.norm.AIC, d.norm.test, type="response"))) +
  labs(y= "actual", x = "predicted") + ggtitle("TEST: actual vs predicted") + 
  geom_point(shape = 1, alpha = 0.7)

glmtrAIC.norm <- ggplot(d.norm.train,aes(y=GA_share, x=predict(glmodel.norm.AIC, d.norm.train, type="response"))) +
  labs(y= "actual", x = "predicted") + ggtitle("TRAIN: actual vs predicted") + 
  geom_point(shape = 1, alpha = 0.7)

gridExtra::grid.arrange(nrow = 1, ncol = 2, 
                        glmteAIC.norm + stat_smooth(size=0.5, method="glm",se=FALSE),
                        glmtrAIC.norm + stat_smooth(size=0.5, method="glm",se=FALSE),
                        top="GLM binomial with NORMALIZED data of SHARE dataset, Best Fit with AIC")

## Evaluating R Square
glmodel.norm.AIC.eval.train <- eval_results(d.norm.train$GA_share, predict(glmodel.norm.AIC, d.norm.train, type="response")) 
glmodel.norm.AIC.eval.test <- eval_results(d.norm.test$GA_share, predict(glmodel.norm.AIC, d.norm.test, type="response"))

writeLines(paste("GLM.norm.AIC: RMSE train: ", round(glmodel.norm.AIC.eval.train$RMSE, 5), 
                 "\nGLM.norm.AIC: Rsquare train: ",round(glmodel.norm.AIC.eval.train$Rsquare, 3),
                 "\nGLM.norm.AIC: RMSE test: ", round(glmodel.norm.AIC.eval.test$RMSE, 5), 
                 "\nGLM.norm.AIC: Rsquare test: ", round(glmodel.norm.AIC.eval.test$Rsquare, 3)))

```

### GLM binomial with ORIGINAL data of COUNT dataset


```{r}
### Modelling GLM with count data:

## Model set up
glmodel.cnt.0 <- glm(cbind(GA_BFS, pop_count_BFS) ~ . -GA_share - HTA_BFS - fn_tck_BFS - 
                   age60.cnt - birth_CH_cnt - female_cnt - hh_6._cnt, 
                family = binomial, data=d.inf_cnt[4:49]) 
# for binomial model, input must be given as one of these 3 categories: 0 or 1, c(success, total), or share with corresponding weights!
summary(glmodel.cnt.0)


# Forward and backwards stepwise!
glmodel.cnt.AIC <- stepAIC(glmodel.cnt.0, direction="both", trace=FALSE)
glmodel.cnt.AIC$formula
glmodel.cnt.AIC$anova
summary(glmodel.cnt.AIC)


# evaluate graphically
glmteAIC.cnt <- ggplot(d.cnt.test, aes(y=GA_share ,x=predict(glmodel.cnt.AIC, d.cnt.test, type="response"))) +
  labs(y= "actual", x = "predicted") + ggtitle("TEST: actual vs predicted") + 
  geom_point(shape = 1, alpha = 0.7)

glmtrAIC.cnt <- ggplot(d.cnt.train,aes(y=GA_share, x=predict(glmodel.cnt.AIC, d.cnt.train, type="response"))) +
  labs(y= "actual", x = "predicted") + ggtitle("TRAIN: actual vs predicted") + 
  geom_point(shape = 1, alpha = 0.7)

gridExtra::grid.arrange(nrow = 1, ncol = 2, 
                        glmteAIC.cnt + stat_smooth(size=0.5, method="glm",se=FALSE),
                        glmtrAIC.cnt + stat_smooth(size=0.5, method="glm",se=FALSE),
                        top="GLM binomial with ORIGINAL data of COUNT dataset, Best fit with AIC")

# ## Evaluating R Square whole model
# glmodel.cnt.0.eval.train <- eval_results(d.cnt.train$GA_share, predict(glmodel.cnt.0, d.cnt.train, type="response")) 
# glmodel.cnt.0.eval.test <- eval_results(d.cnt.test$GA_share, predict(glmodel.cnt.0, d.cnt.test, type="response"))
# 
# 
# 
# writeLines(paste("GLM.cnt.0: RMSE train: ", round(glmodel.cnt.0.eval.train$RMSE, 5), 
#                  "\nGLM.cnt.0: Rsquare train: ",round(glmodel.cnt.0.eval.train$Rsquare, 3),
#                  "\nGLM.cnt.0: RMSE test: ", round(glmodel.cnt.0.eval.test$RMSE, 5), 
#                  "\nGLM.cnt.0: Rsquare test: ", round(glmodel.cnt.0.eval.test$Rsquare, 3)))

## Evaluating R Square best model (AIC)
glmodel.cnt.AIC.eval.train <- eval_results(d.cnt.train$GA_share, predict(glmodel.cnt.AIC, d.cnt.train, type="response")) 
glmodel.cnt.AIC.eval.test <- eval_results(d.cnt.test$GA_share, predict(glmodel.cnt.AIC, d.cnt.test, type="response"))


writeLines(paste("GLM.cnt.AIC: RMSE train: ", round(glmodel.cnt.AIC.eval.train$RMSE, 5), 
                 "\nGLM.cnt.AIC: Rsquare train: ",round(glmodel.cnt.AIC.eval.train$Rsquare, 3),
                 "\nGLM.cnt.AIC: RMSE test: ", round(glmodel.cnt.AIC.eval.test$RMSE, 5), 
                 "\nGLM.cnt.AIC: Rsquare test: ", round(glmodel.cnt.AIC.eval.test$Rsquare, 3)))


```
### GLM binomial with NORMALIZED data of COUNT dataset

```{r}
### Modelling GLM with NORMALIZED count data:

## Model set up
glmodel.cnt.norm <- glm(GA_share ~ . - GA_BFS - HTA_BFS - fn_tck_BFS - 
                   age60.cnt - birth_CH_cnt - female_cnt - hh_6._cnt, 
                family = binomial, data=d.norm.cnt) 

# for binomial model, input must be given as one of these 3 categories: 0 or 1, c(success, total), 3or share with corresponding weights!

summary(glmodel.cnt.norm)


# Forward and backwards stepwise!
glmodel.cnt.norm.AIC <- stepAIC(glmodel.cnt.norm, direction="both", trace=FALSE)
glmodel.cnt.norm.AIC$formula
glmodel.cnt.norm.AIC$anova
summary(glmodel.cnt.norm.AIC)


# evaluate graphically
glmteAIC.norm.cnt <- ggplot(d.norm.cnt.test, aes(y=GA_share ,x=predict(glmodel.cnt.norm.AIC, d.norm.cnt.test, type="response"))) +
  labs(y= "actual", x = "predicted") + ggtitle("TEST: actual vs predicted") + 
  geom_point(shape = 1, alpha = 0.7)

glmtrAIC.norm.cnt <- ggplot(d.norm.cnt.train,aes(y=GA_share, x=predict(glmodel.cnt.norm.AIC, d.norm.cnt.train, type="response"))) +
  labs(y= "actual", x = "predicted") + ggtitle("TRAIN: actual vs predicted") + 
  geom_point(shape = 1, alpha = 0.7)

gridExtra::grid.arrange(nrow = 1, ncol = 2, 
                        glmteAIC.norm.cnt + stat_smooth(size=0.5, method="glm",se=FALSE),
                        glmtrAIC.norm.cnt + stat_smooth(size=0.5, method="glm",se=FALSE),
                        top="GLM binomial with NORMALIZED data of COUNT dataset, Best fit with AIC")

## Evaluating R Square best model (AIC)
glmodel.cnt.norm.AIC.eval.train <- eval_results(d.norm.cnt.train$GA_share, predict(glmodel.cnt.norm.AIC, d.norm.cnt.train, type="response")) 
glmodel.cnt.norm.AIC.eval.test <- eval_results(d.norm.cnt.test$GA_share, predict(glmodel.cnt.norm.AIC, d.norm.cnt.test, type="response"))


writeLines(paste("GLM.cnt.norm.AIC: RMSE train: ", round(glmodel.cnt.norm.AIC.eval.train$RMSE, 5), 
                 "\nGLM.cnt.norm.AIC: Rsquare train: ",round(glmodel.cnt.norm.AIC.eval.train$Rsquare, 3),
                 "\nGLM.cnt.norm.AIC: RMSE test: ", round(glmodel.cnt.norm.AIC.eval.test$RMSE, 5), 
                 "\nGLM.cnt.norm.AIC: Rsquare test: ", round(glmodel.cnt.norm.AIC.eval.test$Rsquare, 3)))


```

## Choosing model

The best performing model seems to be the glmodel.norm.AIC, so this will be taken for further exploration:

```{r}
summary(glmodel.norm.AIC)
sort(abs(glmodel.norm.AIC$coefficients), decreasing = TRUE)
glmodel.norm.AIC$coefficients
```





# CLUSTER ANALYSIS
The best model of the different approaches will result in the highest accuracy and a corresponding list of parameters which are relevant and should be focussed on in the cluster analysis. Many, diverse communities can make it difficult to find easily separable clusters. Therefore, the first step of the cluster analysis is the reduction of the data to the cantonal level. For this purpose, a separate table must be created with a similar procedure as described in chapter 5.2.2, with the shares calculated on this new level of aggregation.
Different options of cluster methods will be integrated in the analysis. I will use an agglomerative clustering model to start, which does not need a pre-defined number of clusters (Ward, 1963). This is an advantage, especially because it is hard to estimate in advance what number of groups will be useful at the end. Alternative models would be a k-means clustering (partitioning method) with a given number of clusters (MacQueen, 1967), a Generalized Mixture Model (GMM) which uses statistical models and is not based simply on a distance measure (Rasmussen, 1999), or even a density-based clustering like DBSCAN which assigns points to clusters based on densities in the data, returning also outliers (Ester et al., 1996).




# Archive


## Linear model
The first method is a classic linear model that models the absolute number of season tickets in circulation. A special eye must be kept on the possible interactions between the variables, wherefore I will include the interactions in the model at the beginning. By using the Akaike Information Criteria (AIC), I can assess afterwards which variables explain the model most meaningfully. Additionally, normalizing data must be taken into account.

### Building linear model with selected variables of original dataset

```{r}
# According to the stepAIC function we get this linear model
lm1 <- lm(GA_share ~ language + married_share + X.20 + X20.40 + 
    X40.60 + birth_munic + birth_cant + birth_CH + male + resid_1.5y + 
    resid_6.10y + hh_1 + hh_2 + hh_3.5 + PT_time_medium + PT_time_big + 
    str_time_medium + str_dist_big + PT_fact_big + PT_fact_medium + 
    bus_stops_per_pop + train_stops_per_pop + bus_stat_per_1000 + 
    train_stat_per_1000 + comb_car_1000 + el_car_1000 + outbound_share, data = d.train[, 
    4:50]) 
summary(lm1)
# summary(log(d.train[,4:50]$pop_count_BFS+1))
```

### Building linear model with all variables of normalized dataset
```{r}
lm.norm.0 <- lm(GA_share ~ . -HTA_share - FNT_share, data = d.norm.train) # Model with all predictors
summary(lm.norm.0) ## We see that there are 4 parameters with NA value => check with alias function
alias(lm.norm.0) # High collinearities between the variables (makes sense!) => Remove these 4 variables from model
lm.norm.01 <- lm(GA_share ~ . - HTA_share - FNT_share - 
                   X.60 - birth_notCH - female - hh_.6, data = d.norm.train)
summary(lm.norm.01) # looks good this time!

MASS::stepAIC(lm.norm.01, direction = "both", trace = FALSE)


# cor(d.norm[,1:46], use="complete.obs")
# barplot(VIF(lm.norm.01))
# VIF_lm_norm <- VIF(lm.norm.01)
# 
# barplot(VIF_lm_norm[,3], ylim=c(0,10))
# data(BODYFAT)
#   M <- lm(BodyFat~.,data=BODYFAT)
#   VIF(M)
```
The stepAIC function gives me now a suggested formula, what can be used for a second try. The coefficients show the weights.

### Building linear model with selected variables of normalized dataset

```{r}
# According to the stepAIC function we get this linear model
lm.norm.1 <- lm(GA_share ~ language + married_share + X.20 + X20.40 + 
    X40.60 + birth_munic + birth_cant + birth_CH + male + resid_1.5y + 
    resid_6.10y + hh_1 + hh_2 + hh_3.5 + PT_time_medium + PT_time_big + 
    str_time_medium + str_dist_big + PT_fact_big + PT_fact_medium + 
    bus_stops_per_pop + train_stops_per_pop + bus_stat_per_1000 + 
    train_stat_per_1000 + comb_car_1000 + el_car_1000 + outbound_share, data = d.norm.train) 
summary(lm.norm.1)
# summary(log(d.train[,4:50]$pop_count_BFS+1))
```




## GLM binomial tests

### GLM binomial with all data of original dataset
With a linear model, no probability distributions can be predicted, as values above 1 and below 0 are possible, resulting in meaningless values concerning the share. Therefore, a second, logistic approach comes into play, which strictly forecasts values between 0 and 1. Although I do not have values for individuals with regard to the share and thus do not model a binary target variable, a multinomial logit model approach is described for aggregated data using group variable, which is in my case the municipality (Morais et al., 2016).
Quasibinomial glm fulfills the criteria here:

See here: https://stats.oarc.ucla.edu/r/dae/multinomial-logistic-regression/#:~:text=Multinomial%20logistic%20regression%20is%20used,the%20examples%20on%20this%20page. 

```{r}
# Setting up model with all data, without the 4 problematic variables

glmodel0 <- glm(GA_share ~ . - HTA_share - FNT_share - 
                   X.60 - birth_notCH - female - hh_.6, 
                family = quasibinomial, data=d.train[,4:50])
summary(glmodel0)

```

The Step AIC function can not be used for the glm as the scale is fixed and can not be used in a maximum-likelihood problem.
I just test the same model as above for the Generalized Linear Model with (quasi-)binomial distribution:


### GLM binomial with selected data of original dataset
```{r}
# Setting up model with selected data

glmodel1 <- glm(GA_share ~ married_share + X.20 + X20.40 + X40.60 + 
    birth_munic + birth_cant + birth_CH + male + resid_.1y + 
    resid_1.5y + resid_6.10y + resid_.10y + hh_1 + hh_2 + hh_3.5 + 
    PT_time_medium + PT_time_big + str_time_medium + str_dist_big + 
    PT_fact_big + PT_fact_medium + bus_stops_per_pop + train_stops_per_pop + 
    bus_stat_per_1000 + train_stat_per_1000 + other_stat_per_1000 + 
    comb_car_1000 + el_car_1000 + inbound_share + outbound_share + 
    language, family = quasibinomial,
    data=d.train[,4:50])
summary(glmodel1)

```


### GLM binomial with reduced selected data of original dataset
Some parameters still do have some depndencies on each other. From each category of explaining variables, I will now extract the one with the strongest influence (strong significant and highest estimate value) for the next model:
```{r}
# Setting up model with selected data

glmodel2 <- glm(GA_share ~ language + X.20 + birth_munic + 
                  resid_6.10y + hh_1 + PT_fact_big + train_stops_per_pop + bus_stat_per_1000 + 
      comb_car_1000, family = quasibinomial, data=d.train[,4:50])
summary(glmodel2)

```




The Step AIC function can not be used for the glm as the scale is fixed and can not be used in a maximum-likelihood problem.
I just test the same model as above for the Generalized Linear Model with (quasi-)binomial distribution:

### GLM binomial with selected data of NORMALIZED dataset
```{r}
# Setting up model with selected data

glmodel.norm.1 <- glm(GA_share ~ language + pop_count_BFS + single_share + 
    married_share + widowed_share + divorced_share + X.20 + X20.40 + 
    X40.60 + birth_munic + birth_cant + birth_CH + male + resid_.1y + 
    resid_1.5y + resid_6.10y + resid_.10y + hh_1 + hh_2 + hh_3.5 + 
    PT_time_medium + PT_time_big + str_time_medium + str_dist_big + 
    PT_fact_big + PT_fact_medium + train_stops_per_pop + bus_stat_per_1000 + 
    train_stat_per_1000 + other_stat_per_1000 + comb_car_1000 + 
    el_car_1000 + inbound_share + outbound_share, 
    family = quasibinomial, data=d.norm.train)
summary(glmodel.norm.1)

```

### GLM binomial with reduced selected data of NORMALIZED dataset
Some parameters still do have some depndencies on each other. From each category of explaining variables, I will now extract the one with the strongest influence (strong significant and highest estimate value) for the next model:
```{r}
# Setting up model with selected data

glmodel.norm.2 <- glm(GA_share ~ language + X.20 + birth_cant + 
                  resid_6.10y + hh_1 + PT_fact_big + train_stops_per_pop + bus_stat_per_1000 + 
      comb_car_1000, family = quasibinomial, data=d.norm.train)
summary(glmodel.norm.2)

```



## Model evaluation

Now it's time to compare the different models to each other.

The comparison between true and predicted for the different models will be presented as well as the RMSE and R square values:

### Graphical evaluation

First the graphically view, comparing the prediction for Test and Train data for final linear model:
```{r warning=FALSE}
mte <- ggplot(d.test, aes(y=GA_share ,x=predict(lm1, d.test))) +
  labs(y= "actual", x = "predicted") + ggtitle("TEST: actual vs predicted") + 
  geom_point(shape = 1, alpha = 0.7)

mtr <- ggplot(d.train,aes(y=GA_share, x=predict(lm1, d.train))) +
  labs(y= "actual", x = "predicted") + ggtitle("TRAIN: actual vs predicted") + 
  geom_point(shape = 1, alpha = 0.7)

gridExtra::grid.arrange(nrow = 1, ncol = 2, mte + 
                        stat_smooth(size=0.5, method="lm",se=FALSE), 
                        mtr + stat_smooth(size=0.5, method="lm",se=FALSE),
                        top = "Linear model, original dataset")


# qplot(d.train$GA_share, predict.lm(lm1, d.train), colour = d.train$language)
# plot(lm1)
```


Now the same for the GLM:

```{r warning=FALSE}
glmte <- ggplot(d.test, aes(y=GA_share ,x=predict(glmodel1, d.test, type="response"))) +
  labs(y= "actual", x = "predicted") + ggtitle("TEST: actual vs predicted") + 
  geom_point(shape = 1, alpha = 0.7)

glmtr <- ggplot(d.train,aes(y=GA_share, x=predict(glmodel1, d.train, type="response"))) +
  labs(y= "actual", x = "predicted") + ggtitle("TRAIN: actual vs predicted") + 
  geom_point(shape = 1, alpha = 0.7)

gridExtra::grid.arrange(nrow = 1, ncol = 2, 
                        glmte + stat_smooth(size=0.5, method="glm",se=FALSE),
                        glmtr + stat_smooth(size=0.5, method="glm",se=FALSE),
                        top="GLM Quasibinomial model 1. original dataset")

# qplot(d.train$GA_share, predict(glmodel1, d.train, type="response"), colour = d.train$language)
# plot(glmodel1)

```
Now the same for the GLM 2 :

```{r warning=FALSE}
glmte2 <- ggplot(d.test, aes(y=GA_share ,x=predict(glmodel2, d.test, type="response"))) +
  labs(y= "actual", x = "predicted") + ggtitle("TEST: actual vs predicted") + 
  geom_point(shape = 1, alpha = 0.7)

glmtr2 <- ggplot(d.train,aes(y=GA_share, x=predict(glmodel2, d.train, type="response"))) +
  labs(y= "actual", x = "predicted") + ggtitle("TRAIN: actual vs predicted") + 
  geom_point(shape = 1, alpha = 0.7)

gridExtra::grid.arrange(nrow = 1, ncol = 2, 
                        glmte2 + stat_smooth(size=0.5, method="glm",se=FALSE),
                        glmtr2 + stat_smooth(size=0.5, method="glm",se=FALSE),
                        top="GLM Quasibinomial model 2, original dataset")

# qplot(d.train$GA_share, predict(glmodel1, d.train, type="response"), colour = d.train$language)
# plot(glmodel1)

```
Now the whole thing again for the normalized dataset:


First the graphically view, comparing the prediction for Test and Train data for final linear model:
```{r warning=FALSE}
mte.norm <- ggplot(d.norm.test, aes(y=GA_share ,x=predict(lm.norm.1, d.norm.test))) +
  labs(y= "actual", x = "predicted") + ggtitle("TEST: actual vs predicted") + 
  geom_point(shape = 1, alpha = 0.7)

mtr.norm <- ggplot(d.norm.train,aes(y=GA_share, x=predict(lm.norm.1, d.norm.train))) +
  labs(y= "actual", x = "predicted") + ggtitle("TRAIN: actual vs predicted") + 
  geom_point(shape = 1, alpha = 0.7)

gridExtra::grid.arrange(nrow = 1, ncol = 2, mte.norm + 
                        stat_smooth(size=0.5, method="lm",se=FALSE), 
                        mtr.norm + stat_smooth(size=0.5, method="lm",se=FALSE),
                        top = "Linear model, Normalized dataset")


# qplot(d.train$GA_share, predict.lm(lm1, d.train), colour = d.train$language)
# plot(lm1)
```


Now the same for the GLM:

```{r}
glmteAIC.norm <- ggplot(d.norm.test, aes(y=GA_share ,x=predict(glmodel.norm.AIC, d.norm.test, type="response"))) +
  labs(y= "actual", x = "predicted") + ggtitle("TEST: actual vs predicted") + 
  geom_point(shape = 1, alpha = 0.7)

glmtrAIC.norm <- ggplot(d.norm.train,aes(y=GA_share, x=predict(glmodel.norm.AIC, d.norm.train, type="response"))) +
  labs(y= "actual", x = "predicted") + ggtitle("TRAIN: actual vs predicted") + 
  geom_point(shape = 1, alpha = 0.7)

gridExtra::grid.arrange(nrow = 1, ncol = 2, 
                        glmteAIC.norm + stat_smooth(size=0.5, method="glm",se=FALSE),
                        glmtrAIC.norm + stat_smooth(size=0.5, method="glm",se=FALSE),
                        top="GLM Quasibinomial model 0, Normalized dataset")
```



```{r warning=FALSE}
glmte.norm <- ggplot(d.norm.test, aes(y=GA_share ,x=predict(glmodel.norm.1, d.norm.test, type="response"))) +
  labs(y= "actual", x = "predicted") + ggtitle("TEST: actual vs predicted") + 
  geom_point(shape = 1, alpha = 0.7)

glmtr.norm <- ggplot(d.norm.train,aes(y=GA_share, x=predict(glmodel.norm.1, d.norm.train, type="response"))) +
  labs(y= "actual", x = "predicted") + ggtitle("TRAIN: actual vs predicted") + 
  geom_point(shape = 1, alpha = 0.7)

gridExtra::grid.arrange(nrow = 1, ncol = 2, 
                        glmte.norm + stat_smooth(size=0.5, method="glm",se=FALSE),
                        glmtr.norm + stat_smooth(size=0.5, method="glm",se=FALSE),
                        top="GLM Quasibinomial model 1, Normalized dataset")

# qplot(d.train$GA_share, predict(glmodel1, d.train, type="response"), colour = d.train$language)
# plot(glmodel1)

```
Now the same for the GLM 2 :

```{r warning=FALSE}
glmte.norm.2 <- ggplot(d.norm.test, aes(y=GA_share ,x=predict(glmodel.norm.2, d.norm.test, type="response"))) +
  labs(y= "actual", x = "predicted") + ggtitle("TEST: actual vs predicted") + 
  geom_point(shape = 1, alpha = 0.7)

glmtr.norm.2 <- ggplot(d.norm.train,aes(y=GA_share, x=predict(glmodel.norm.2, d.norm.train, type="response"))) +
  labs(y= "actual", x = "predicted") + ggtitle("TRAIN: actual vs predicted") + 
  geom_point(shape = 1, alpha = 0.7)

gridExtra::grid.arrange(nrow = 1, ncol = 2, 
                        glmte.norm.2 + stat_smooth(size=0.5, method="glm",se=FALSE),
                        glmtr.norm.2 + stat_smooth(size=0.5, method="glm",se=FALSE),
                        top="GLM Quasibinomial model 2, original dataset")

# qplot(d.train$GA_share, predict(glmodel1, d.train, type="response"), colour = d.train$language)
# plot(glmodel1)

```





### Evaluation results

#### Original dataset
Now the comparison of the RMSE and Rsquare values:

First the values for the linear model, which takes all variables into account:

```{r warning=FALSE}
lm0.eval.train <- eval_results(d.train$GA_share, predict.lm(lm0, d.train))
lm0.eval.test <- eval_results(d.test$GA_share, predict.lm(lm0, d.test))

writeLines(paste("Lm0: RMSE train: ", round(lm0.eval.train$RMSE, 4),
                 "\nLm0: Rsquare train: ",round(lm0.eval.train$Rsquare, 3),
                 "\nLm0: RMSE test: ", round(lm0.eval.test$RMSE, 4),
                 "\nLm0: Rsquare test: ", round(lm0.eval.test$Rsquare, 3)))


# qplot(d.train$GA_share, predict.lm(lm0, d.train), colour = d.train$language)
```

Second, the values for the linear model, which takes only selected variables into account:
```{r}
lm1.eval.train <- eval_results(d.train$GA_share, predict.lm(lm1, d.train)) 
lm1.eval.test <- eval_results(d.test$GA_share, predict.lm(lm1, d.test))

writeLines(paste("Lm1: RMSE train: ", round(lm1.eval.train$RMSE, 4), 
                 "\nLm1: Rsquare train: ",round(lm1.eval.train$Rsquare, 3), 
                 "\nLm1: RMSE test: ", round(lm1.eval.test$RMSE, 4), 
                 "\nLm1: Rsquare test: ", round(lm1.eval.test$Rsquare, 3)))
```

And second the values for the GLM:
```{r warning=FALSE}
glmodel0.eval.train <- eval_results(d.train[,4:50]$GA_share, predict(glmodel0, d.train[,4:50], type="response")) 
glmodel0.eval.test <- eval_results(d.test[,4:50]$GA_share, predict(glmodel0, d.test[,4:50], type="response"))

writeLines(paste("GLM0: RMSE train: ", round(glmodel0.eval.train$RMSE, 5), 
                 "\nGLM0: Rsquare train: ",round(glmodel0.eval.train$Rsquare, 3),
                 "\nGLM0: RMSE test: ", round(glmodel0.eval.test$RMSE, 5), 
                 "\nGLM0: Rsquare test: ", round(glmodel0.eval.test$Rsquare, 3)))
```

```{r}
glmodel1.eval.train <- eval_results(d.train[,4:50]$GA_share, predict(glmodel1, d.train[,4:50], type="response")) 
glmodel1.eval.test <- eval_results(d.test[,4:50]$GA_share, predict(glmodel1, d.test[,4:50], type="response"))

writeLines(paste("GLM1: RMSE train: ", round(glmodel1.eval.train$RMSE, 5), 
                 "\nGLM1: Rsquare train: ",round(glmodel1.eval.train$Rsquare, 3),
                 "\nGLM1: RMSE test: ", round(glmodel1.eval.test$RMSE, 5), 
                 "\nGLM1: Rsquare test: ", round(glmodel1.eval.test$Rsquare, 3)))
```


```{r}
glmodel2.eval.train <- eval_results(d.train[,4:50]$GA_share, predict(glmodel2, d.train[,4:50], type="response")) 
glmodel2.eval.test <- eval_results(d.test[,4:50]$GA_share, predict(glmodel2, d.test[,4:50], type="response"))

writeLines(paste("GLM2: RMSE train: ", round(glmodel2.eval.train$RMSE, 4), 
                 "\nGLM2: Rsquare train: ",round(glmodel2.eval.train$Rsquare, 3),
                 "\nGLM2: RMSE test: ", round(glmodel2.eval.test$RMSE, 4), 
                 "\nGLM2: Rsquare test: ", round(glmodel2.eval.test$Rsquare, 3)))
```
#### Normalized dataset
Now the comparison of the RMSE and Rsquare values:

First the values for the linear model, which takes all variables into account:

```{r warning=FALSE}
lm.norm.0.eval.train <- eval_results(d.norm.train$GA_share, predict.lm(lm.norm.0, d.norm.train))
lm.norm.0.eval.test <- eval_results(d.norm.test$GA_share, predict.lm(lm.norm.0, d.norm.test))

writeLines(paste("Lm.norm.0: RMSE train: ", round(lm.norm.0.eval.train$RMSE, 4),
                 "\nLm.norm.0: Rsquare train: ",round(lm.norm.0.eval.train$Rsquare, 3),
                 "\nLm.norm.0: RMSE test: ", round(lm.norm.0.eval.test$RMSE, 4),
                 "\nLm.norm.0: Rsquare test: ", round(lm.norm.0.eval.test$Rsquare, 3)))
```

Second, the values for the linear model, which takes only selected variables into account:
```{r}
lm.norm.1.eval.train <- eval_results(d.norm.train$GA_share, predict.lm(lm.norm.1, d.norm.train))
lm.norm.1.eval.test <- eval_results(d.norm.test$GA_share, predict.lm(lm.norm.1, d.norm.test))

writeLines(paste("Lm.norm.1: RMSE train: ", round(lm.norm.1.eval.train$RMSE, 4),
                 "\nLm.norm.1: Rsquare train: ",round(lm.norm.1.eval.train$Rsquare, 3),
                 "\nLm.norm.1: RMSE test: ", round(lm.norm.1.eval.test$RMSE, 4),
                 "\nLm.norm.1: Rsquare test: ", round(lm.norm.1.eval.test$Rsquare, 3)))
```

And second the values for the GLM:
```{r warning=FALSE}
glmodel.norm.AIC.eval.train <- eval_results(d.norm.train$GA_share, predict(glmodel.norm.AIC, d.norm.train, type="response")) 
glmodel.norm.AIC.eval.test <- eval_results(d.norm.test$GA_share, predict(glmodel.norm.AIC, d.norm.test, type="response"))

writeLines(paste("GLM.norm.0: RMSE train: ", round(glmodel.norm.AIC.eval.train$RMSE, 5), 
                 "\nGLM.norm.0: Rsquare train: ",round(glmodel.norm.AIC.eval.train$Rsquare, 3),
                 "\nGLM.norm.0: RMSE test: ", round(glmodel.norm.AIC.eval.test$RMSE, 5), 
                 "\nGLM.norm.0: Rsquare test: ", round(glmodel.norm.AIC.eval.test$Rsquare, 3)))
```

```{r}
glmodel.norm.1.eval.train <- eval_results(d.norm.train$GA_share, predict(glmodel.norm.1, d.norm.train, type="response")) 
glmodel.norm.1.eval.test <- eval_results(d.norm.test$GA_share, predict(glmodel.norm.1, d.norm.test, type="response"))

writeLines(paste("GLM.norm.1: RMSE train: ", round(glmodel.norm.1.eval.train$RMSE, 4), 
                 "\nGLM.norm.1: Rsquare train: ",round(glmodel.norm.1.eval.train$Rsquare, 3),
                 "\nGLM.norm.1: RMSE test: ", round(glmodel.norm.1.eval.test$RMSE, 4), 
                 "\nGLM.norm.1: Rsquare test: ", round(glmodel.norm.1.eval.test$Rsquare, 3)))
```



```{r}
glmodel.norm.2.eval.train <- eval_results(d.norm.train$GA_share, predict(glmodel.norm.2, d.norm.train, type="response")) 
glmodel.norm.2.eval.test <- eval_results(d.norm.test$GA_share, predict(glmodel.norm.2, d.norm.test, type="response"))

writeLines(paste("GLM.norm.2: RMSE train: ", round(glmodel.norm.2.eval.train$RMSE, 4), 
                 "\nGLM.norm.2: Rsquare train: ",round(glmodel.norm.2.eval.train$Rsquare, 3),
                 "\nGLM.norm.2: RMSE test: ", round(glmodel.norm.2.eval.test$RMSE, 4), 
                 "\nGLM.norm.2: Rsquare test: ", round(glmodel.norm.2.eval.test$Rsquare, 3)))
```
