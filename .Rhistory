adj_r2(lm.count.FNT))
## Evaluating LM01 (original lm with all data, DE-LOGARITHMIZED)
model_evaluation["FNT_LM_01_cnt",] <- c(eval_results(exp(d.norm.count.train$fn_tck_BFS),
exp(predict(lm.count.FNT, d.norm.count.train,
type="response"))),
eval_results(exp(d.norm.count.test$fn_tck_BFS),
exp(predict(lm.count.FNT, d.norm.count.test,
type="response"))),
lm.count.FNT$rank,
adj_r2(lm.count.FNT))
## Evaluating LM02 (LM after stepwise parameter selection, LOG-TRANSFORMED)
model_evaluation["FNT_LM_02_log",] <- c(eval_results(d.norm.count.train$fn_tck_BFS,
predict(lm.count.FNT2, d.norm.count.train,
type="response")),
eval_results(d.norm.count.test$fn_tck_BFS,
predict(lm.count.FNT2, d.norm.count.test,
type="response")),
lm.count.FNT2$rank,
adj_r2(lm.count.FNT2))
## Evaluating LM02 (LM after stepwise parameter selection, DE-LOGARITHMIZED)
model_evaluation["FNT_LM_02_cnt",] <- c(eval_results(exp(d.norm.count.train$fn_tck_BFS),
exp(predict(lm.count.FNT2, d.norm.count.train,
type="response"))),
eval_results(exp(d.norm.count.test$fn_tck_BFS),
exp(predict(lm.count.FNT2, d.norm.count.test,
type="response"))),
lm.count.FNT2$rank,
adj_r2(lm.count.FNT2))
# delete the first empty entry in the table:
model_evaluation <- model_evaluation[-1,]
round(model_evaluation, digits=2)
# evaluate graphically with Step AIC
glm_ga_01_te <- ggplot(d.norm.share.test, aes(y=GA_share ,x=predict(glm.share.01,
d.norm.share.test, type="response"))) +
labs(y= "actual", x = "predicted") + ggtitle("GA_GLM_01 TEST") +
geom_point(shape = 1, alpha = 0.7) +
theme(plot.title = element_text(size=8), axis.text = element_text(size=7),
axis.title = element_text(size=7))
glm_ga_01_tr <- ggplot(d.norm.share.train,aes(y=GA_share, x=predict(glm.share.01,
d.norm.share.train, type="response"))) +
labs(y= "actual", x = "predicted") + ggtitle("GA_GLM_01 TRAIN") +
geom_point(shape = 1, alpha = 0.7, colour = "darkblue") +
theme(plot.title = element_text(size=8, colour="darkblue"), axis.text = element_text(size=7),
axis.title = element_text(size=7))
glm_ga_03_te <- ggplot(d.norm.share.test, aes(y=GA_share ,x=predict(glm.share.03,
d.norm.share.test, type="response"))) +
labs(y= "actual", x = "predicted") + ggtitle("GA_GLM_03 TEST") +
geom_point(shape = 1, alpha = 0.7) +
theme(plot.title = element_text(size=8), axis.text = element_text(size=7),
axis.title = element_text(size=7))
glm_ga_03_tr <- ggplot(d.norm.share.train,aes(y=GA_share, x=predict(glm.share.03,
d.norm.share.train, type="response"))) +
labs(y= "actual", x = "predicted") + ggtitle("GA_GLM_03 TRAIN") +
geom_point(shape = 1, alpha = 0.7, colour = "darkblue") +
theme(plot.title = element_text(size=8, colour="darkblue"), axis.text = element_text(size=7),
axis.title = element_text(size=7))
glm_ga_04_te <- ggplot(d.norm.share.test, aes(y=GA_share ,x=predict(glm.share.04,
d.norm.share.test, type="response"))) +
labs(y= "actual", x = "predicted") + ggtitle("GA_GLM_04 TEST") +
geom_point(shape = 1, alpha = 0.7) +
theme(plot.title = element_text(size=8), axis.text = element_text(size=7),
axis.title = element_text(size=7))
glm_ga_04_tr <- ggplot(d.norm.share.train,aes(y=GA_share, x=predict(glm.share.04,
d.norm.share.train, type="response"))) +
labs(y= "actual", x = "predicted") + ggtitle("GA_GLM_04 TRAIN") +
geom_point(shape = 1, alpha = 0.7, colour = "darkblue") +
theme(plot.title = element_text(size=8, colour="darkblue"), axis.text = element_text(size=7),
axis.title = element_text(size=7))
glm_ga_05_te <- ggplot(d.norm.share.test, aes(y=GA_share ,x=predict(glm.share.05,
d.norm.share.test, type="response"))) +
labs(y= "actual", x = "predicted") + ggtitle("GA_GLM_05 TEST") +
geom_point(shape = 1, alpha = 0.7) +
theme(plot.title = element_text(size=8), axis.text = element_text(size=7),
axis.title = element_text(size=7))
glm_ga_05_tr <- ggplot(d.norm.share.train,aes(y=GA_share, x=predict(glm.share.05,
d.norm.share.train, type="response"))) +
labs(y= "actual", x = "predicted") + ggtitle("GA_GLM_05 TRAIN") +
geom_point(shape = 1, alpha = 0.7, colour = "darkblue") +
theme(plot.title = element_text(size=8, colour="darkblue"), axis.text = element_text(size=7),
axis.title = element_text(size=7))
gridExtra::grid.arrange(nrow = 2, ncol = 4,
glm_ga_01_te + stat_smooth(size=0.5, method="glm",se=FALSE),
glm_ga_01_tr + stat_smooth(size=0.5, method="glm",se=FALSE, colour="black"),
glm_ga_03_te + stat_smooth(size=0.5, method="glm",se=FALSE),
glm_ga_03_tr + stat_smooth(size=0.5, method="glm",se=FALSE, colour="black"),
glm_ga_04_te + stat_smooth(size=0.5, method="glm",se=FALSE),
glm_ga_04_tr + stat_smooth(size=0.5, method="glm",se=FALSE, colour="black"),
glm_ga_05_te + stat_smooth(size=0.5, method="glm",se=FALSE),
glm_ga_05_tr + stat_smooth(size=0.5, method="glm",se=FALSE, colour="black"),
top="GA: GRAPHICAL MODEL COMPARISON of different GLM")
# evaluate graphically with Step AIC
glm_hta_01_te <- ggplot(d.norm.share.test, aes(y=HTA_share ,x=predict(glm.HTA.share.01,
d.norm.share.test, type="response"))) +
labs(y= "actual", x = "predicted") + ggtitle("HTA_GLM_01 TEST") +
geom_point(shape = 1, alpha = 0.7) +
theme(plot.title = element_text(size=8), axis.text = element_text(size=7),
axis.title = element_text(size=7))
glm_hta_01_tr <- ggplot(d.norm.share.train,aes(y=HTA_share, x=predict(glm.HTA.share.01,
d.norm.share.train, type="response"))) +
labs(y= "actual", x = "predicted") + ggtitle("HTA_GLM_01 TRAIN") +
geom_point(shape = 1, alpha = 0.7, colour = "darkblue") +
theme(plot.title = element_text(size=8, colour="darkblue"), axis.text = element_text(size=7),
axis.title = element_text(size=7))
glm_hta_02_te <- ggplot(d.norm.share.test, aes(y=HTA_share ,x=predict(glm.HTA.share.02,
d.norm.share.test, type="response"))) +
labs(y= "actual", x = "predicted") + ggtitle("HTA_GLM_02 TEST") +
geom_point(shape = 1, alpha = 0.7) +
theme(plot.title = element_text(size=8), axis.text = element_text(size=7),
axis.title = element_text(size=7))
glm_hta_02_tr <- ggplot(d.norm.share.train,aes(y=HTA_share, x=predict(glm.HTA.share.02,
d.norm.share.train, type="response"))) +
labs(y= "actual", x = "predicted") + ggtitle("HTA_GLM_02 TRAIN") +
geom_point(shape = 1, alpha = 0.7, colour = "darkblue") +
theme(plot.title = element_text(size=8, colour="darkblue"), axis.text = element_text(size=7),
axis.title = element_text(size=7))
glm_hta_03_te <- ggplot(d.norm.share.test, aes(y=HTA_share ,x=predict(glm.HTA.share.03,
d.norm.share.test, type="response"))) +
labs(y= "actual", x = "predicted") + ggtitle("HTA_GLM_03 TEST") +
geom_point(shape = 1, alpha = 0.7) +
theme(plot.title = element_text(size=8), axis.text = element_text(size=7),
axis.title = element_text(size=7))
glm_hta_03_tr <- ggplot(d.norm.share.train,aes(y=HTA_share, x=predict(glm.HTA.share.03,
d.norm.share.train, type="response"))) +
labs(y= "actual", x = "predicted") + ggtitle("HTA_GLM_03 TRAIN") +
geom_point(shape = 1, alpha = 0.7, colour = "darkblue") +
theme(plot.title = element_text(size=8, colour="darkblue"), axis.text = element_text(size=7),
axis.title = element_text(size=7))
glm_hta_04_te <- ggplot(d.norm.share.test, aes(y=HTA_share ,x=predict(glm.HTA.share.04,
d.norm.share.test, type="response"))) +
labs(y= "actual", x = "predicted") + ggtitle("HTA_GLM_04 TEST") +
geom_point(shape = 1, alpha = 0.7) +
theme(plot.title = element_text(size=8), axis.text = element_text(size=7),
axis.title = element_text(size=7))
glm_hta_04_tr <- ggplot(d.norm.share.train,aes(y=HTA_share, x=predict(glm.HTA.share.04,
d.norm.share.train, type="response"))) +
labs(y= "actual", x = "predicted") + ggtitle("HTA_GLM_04 TRAIN") +
geom_point(shape = 1, alpha = 0.7, colour = "darkblue") +
theme(plot.title = element_text(size=8, colour="darkblue"), axis.text = element_text(size=7),
axis.title = element_text(size=7))
gridExtra::grid.arrange(nrow = 2, ncol = 4,
glm_hta_01_te + stat_smooth(size=0.5, method="glm",se=FALSE),
glm_hta_01_tr + stat_smooth(size=0.5, method="glm",se=FALSE, colour="black"),
glm_hta_02_te + stat_smooth(size=0.5, method="glm",se=FALSE),
glm_hta_02_tr + stat_smooth(size=0.5, method="glm",se=FALSE, colour="black"),
glm_hta_03_te + stat_smooth(size=0.5, method="glm",se=FALSE),
glm_hta_03_tr + stat_smooth(size=0.5, method="glm",se=FALSE, colour="black"),
glm_hta_04_te + stat_smooth(size=0.5, method="glm",se=FALSE),
glm_hta_04_tr + stat_smooth(size=0.5, method="glm",se=FALSE, colour="black"),
top="HTA: GRAPHICAL MODEL COMPARISON of different GLM")
# evaluate graphically with Step AIC
glm_fnt_01_te <- ggplot(d.norm.share.test, aes(y=FNT_share ,x=predict(glm.FNT.share.01,
d.norm.share.test, type="response"))) +
labs(y= "actual", x = "predicted") + ggtitle("FNT_GLM_01 TEST") +
geom_point(shape = 1, alpha = 0.7) +
theme(plot.title = element_text(size=8), axis.text = element_text(size=7),
axis.title = element_text(size=7))
glm_fnt_01_tr <- ggplot(d.norm.share.train,aes(y=FNT_share, x=predict(glm.FNT.share.01,
d.norm.share.train, type="response"))) +
labs(y= "actual", x = "predicted") + ggtitle("FNT_GLM_01 TRAIN") +
geom_point(shape = 1, alpha = 0.7, colour = "darkblue") +
theme(plot.title = element_text(size=8, colour="darkblue"), axis.text = element_text(size=7),
axis.title = element_text(size=7))
glm_fnt_02_te <- ggplot(d.norm.share.test, aes(y=FNT_share ,x=predict(glm.FNT.share.02,
d.norm.share.test, type="response"))) +
labs(y= "actual", x = "predicted") + ggtitle("FNT_GLM_02 TEST") +
geom_point(shape = 1, alpha = 0.7) +
theme(plot.title = element_text(size=8), axis.text = element_text(size=7),
axis.title = element_text(size=7))
glm_fnt_02_tr <- ggplot(d.norm.share.train,aes(y=FNT_share, x=predict(glm.FNT.share.02,
d.norm.share.train, type="response"))) +
labs(y= "actual", x = "predicted") + ggtitle("FNT_GLM_02 TRAIN") +
geom_point(shape = 1, alpha = 0.7, colour = "darkblue") +
theme(plot.title = element_text(size=8, colour="darkblue"), axis.text = element_text(size=7),
axis.title = element_text(size=7))
glm_fnt_03_te <- ggplot(d.norm.share.test, aes(y=FNT_share ,x=predict(glm.FNT.share.03,
d.norm.share.test, type="response"))) +
labs(y= "actual", x = "predicted") + ggtitle("FNT_GLM_03 TEST") +
geom_point(shape = 1, alpha = 0.7) +
theme(plot.title = element_text(size=8), axis.text = element_text(size=7),
axis.title = element_text(size=7))
glm_fnt_03_tr <- ggplot(d.norm.share.train,aes(y=FNT_share, x=predict(glm.FNT.share.03,
d.norm.share.train, type="response"))) +
labs(y= "actual", x = "predicted") + ggtitle("FNT_GLM_03 TRAIN") +
geom_point(shape = 1, alpha = 0.7, colour = "darkblue") +
theme(plot.title = element_text(size=8, colour="darkblue"), axis.text = element_text(size=7),
axis.title = element_text(size=7))
glm_fnt_04_te <- ggplot(d.norm.share.test, aes(y=FNT_share ,x=predict(glm.FNT.share.04,
d.norm.share.test, type="response"))) +
labs(y= "actual", x = "predicted") + ggtitle("FNT_GLM_04 TEST") +
geom_point(shape = 1, alpha = 0.7) +
theme(plot.title = element_text(size=8), axis.text = element_text(size=7),
axis.title = element_text(size=7))
glm_fnt_04_tr <- ggplot(d.norm.share.train,aes(y=FNT_share, x=predict(glm.FNT.share.04,
d.norm.share.train, type="response"))) +
labs(y= "actual", x = "predicted") + ggtitle("FNT_GLM_04 TRAIN") +
geom_point(shape = 1, alpha = 0.7, colour = "darkblue") +
theme(plot.title = element_text(size=8, colour="darkblue"), axis.text = element_text(size=7),
axis.title = element_text(size=7))
gridExtra::grid.arrange(nrow = 2, ncol = 4,
glm_fnt_01_te + stat_smooth(size=0.5, method="glm",se=FALSE),
glm_fnt_01_tr + stat_smooth(size=0.5, method="glm",se=FALSE, colour="black"),
glm_fnt_02_te + stat_smooth(size=0.5, method="glm",se=FALSE),
glm_fnt_02_tr + stat_smooth(size=0.5, method="glm",se=FALSE, colour="black"),
glm_fnt_03_te + stat_smooth(size=0.5, method="glm",se=FALSE),
glm_fnt_03_tr + stat_smooth(size=0.5, method="glm",se=FALSE, colour="black"),
glm_fnt_04_te + stat_smooth(size=0.5, method="glm",se=FALSE),
glm_fnt_04_tr + stat_smooth(size=0.5, method="glm",se=FALSE, colour="black"),
top="FNT: GRAPHICAL MODEL COMPARISON of different GLM")
ga.coef <- data.frame(as.list(glm.share.05$coefficients))
hta.coef <- data.frame(as.list(glm.HTA.share.04$coefficients))
fnt.coef <- data.frame(as.list(glm.FNT.share.04$coefficients))
inf_factors <- rbind.fill(ga.coef, hta.coef, fnt.coef)
inf_factors <- t(inf_factors)
colnames(inf_factors) <- c("GA_share", "HTA_share", "FNT_share")
### Write csv
write.csv(inf_factors, file="inf_factors.csv")
lowerFn <- function(data, mapping, method = "lm", ...) {
p <- ggplot(data = data, mapping = mapping) +
geom_point(colour = "blue", size=1.2) +
geom_smooth(method = method, color = "black", ...)
p
}
ggpairs(data.frame(inf_factors[2:32,]),
lower = list(continuous= wrap(lowerFn, method = "lm")),
diag = list(continuous = wrap("barDiag", colour = "blue")),
upper = list(continuous = wrap("cor", size = 6))) +
ggtitle("Comparison of MODEL ESTIMATES for the GA-, HTA- and FNT-model")
ggpairs(data.frame(d.share[,c("GA_share", "HTA_share", "FNT_share")]),
lower = list(continuous= wrap(lowerFn, method = "lm")),
diag = list(continuous = wrap("barDiag", colour = "blue")),
upper = list(continuous = wrap("cor", size = 6))) +
ggtitle("Comparison of SHARE VALUES for GA, HTA and FNT")
rownames(inf_factors)
# filtering out language (loading separate) and interaction terms
cl.an.input <- d.norm.share[,c("language", rownames(inf_factors)
[c(2:17, 21, 24:29, 31)])]
# Distance matrix
dp <- dist(cl.an.input[,2:25]) ## Euclidean distance
# Principal components
PC.inf.fac <- princomp(cl.an.input[,2:25]) # without language
summary(PC.inf.fac, loadings = FALSE)
# first 2 PCs explain approx. 30% of the variance, 5 PC about 54%
plot(PC.inf.fac, main = "Principal components: Explained variances per PC")
# write two first principal components out
pc <- PC.inf.fac$scores[,1:2]
## Agglomerative clustering with complete linkage
cl.inf.fac <- hclust(dp, method = "complete")
## Show dendrogram
plot(cl.inf.fac)
abline(h=14, col="blue")
## Split into groups
k = 18 # with 18 clusters, at least 2 big groups!
grps <- cutree(cl.inf.fac, k = k)
## Visualize result in PC 1 & 2
plot(pc, pch = grps, col=grps, lwd=2, main="Cluster result agglomerative clustering")
legend("bottomright", legend = 1:k, pch = 1:k, col=1:k, bty="n", cex=0.85)
## Look at means of clusters / centroids
# aggregate(cl.an.input, by=list(cluster=grps), mean)
## Silhouette plot from package "cluster"
summary(cluster::silhouette(grps, dp))
# k-means
## Choose number of clusters k with scree plot
nr <- 12
wss <- rep(0, nr)
for (i in 1:nr) wss[i] <- sum(kmeans(cl.an.input[,2:25], centers = i, nstart = 20)$withinss)
par(mfrow = c(1,1))
plot(1:nr, wss, type = "b", xlab = "Number of groups", ylab = "Within groups sum of squares")
seed = 123
for(k in 2:10) {
seed = 123
## k-means with 3-10 centers
ckm <- kmeans(cl.an.input[,2:25], centers = k, nstart = 10)
grpsKM <- ckm$cluster
## Silhouette Summary
print(paste("k means clustering with ", k, "clusters: Individual silhouette widths:"))
print(summary(silhouette(grpsKM, dp))[1])
}
## k-means with 2 centers
ckm2 <- kmeans(cl.an.input[,2:25], centers = 2, nstart = 10)
grpsKM2<- ckm2$cluster
## k-means with 6 centers
ckm6 <- kmeans(cl.an.input[,2:25], centers = 6, nstart = 10)
grpsKM6<- ckm6$cluster
###############################
## Gaussian Mixture Models ####
###############################
mc <- Mclust(cl.an.input[,2:25]) # takes long time!
table(mc$classification)
# DBSCAN
kNNdistplot(cl.an.input[,2:25], k = 26) ## -> k = dim + 1; eps value = 6 (sharp increase)
abline(h=6, col = "red", lty=2)
# setting parameters according to minPts = k and eps = sharp increase
minPts=26 # = k
eps=6 # cutting line above
dbs <- dbscan(cl.an.input[,2:25], eps = eps, minPts = minPts)
max(dbs$cluster) # 1
# Searching for better parameters:
## For eps value (i), only values from 1 to 5 are tested, because higher values
## always lead to only one cluster at the end.
# For minPts, the range from 2 to 5 is tested. 1 would lead to as many clusters
# as points are and for more than 5, only one cluster can be obtained.
for(i in (1:5)) {
for(j in (2:5)) {
dbs <- dbscan(cl.an.input[,2:25], eps = i, minPts = j)
print(paste("minPts:", j, "; eps: ", i, "; nr. of clusters: ", max(dbs$cluster))) # 1
print(table(dbs$cluster))
}
}
## PLOT CLUSTER CALSSES ON PC 1 & 2
par(mfrow=c(2,2), mar=c(2,2,1.5,1))
# Gaussian Mixture model
plot(pc, pch = mc$classification, col = mc$classification,
main="Gaussian Mixture Model (4 clusters)")
legend("bottomright", legend = 1:4, pch = 1:4, col=1:4, bty="n")
# K-means with 2 clusters
plot(pc, pch = grpsKM2, col=grpsKM2, main="K-means (2 clusters)")
legend("bottomright", legend = 1:2, pch = 1:2, col=1:2, bty="n")
# K-means with 6 clusters
plot(pc, pch = grpsKM6, col=grpsKM6, main="K-means (6 clusters)")
legend("bottomright", legend = 1:6, pch = 1:6, col=1:5, bty="n")
# Agglomerative clustering
plot(pc, pch = grps, col=grps, main="Agglomerative clustering (18 clusters)")
legend("bottomright", legend = 1:18, pch = 1:18, col=1:18, bty="n", cex=0.5)
## SILHOUETTE PLOTS ##
## Reduce number of data to see a sample of the whole dataset
# classifications
grps.ss <- grps[1:200]
mc.ss <- mc$classification[1:200]
grpsKM2.ss <- grpsKM2[1:200]
grpsKM6.ss <- grpsKM6[1:200]
# distance table
dp.ss <- dist(cl.an.input[1:200,2:25]) ## Euclidean
# Silhouette Plot from package "cluster"
par(mfrow=c(1,2), mar=c(5,4,4,2))
plot(silhouette(grps.ss, dp.ss, cex=0.8), main="Agglomerative clustering")
plot(silhouette(grpsKM2.ss, dp.ss), main="k-means clustering (k=2)")
plot(silhouette(grpsKM6.ss, dp.ss), main="k-means clustering (k=6)")
plot(silhouette(mc.ss, dp.ss), main="Gaussian mixture model")
## GROUPS REPRESENTING LANGUAGE REGIONS
ca.aggl.lan <- data.frame(cls = grps, lang = cl.an.input[,1])
table(ca.aggl.lan)
ca.km2.lan <- data.frame(cls = grpsKM2, lang = cl.an.input[,1])
table(ca.km2.lan)
ca.km6.lan <- data.frame(cls = grpsKM6, lang = cl.an.input[,1])
table(ca.km6.lan)
ca.mc.lan <- data.frame(cls = mc$classification, lang = cl.an.input[,1])
table(ca.mc.lan)
# filtering out language (loading separate) and interaction terms
d.share.clust <- d.share[,c("language", rownames(inf_factors)[c(2:17, 21, 24:29, 31)])]
d.share.clust["k_cluster"] = grpsKM6
d.share.clust["GA_share"] = d.share$GA_share
d.share.clust["HTA_share"] = d.share$HTA_share
d.share.clust["FNT_share"] = d.share$FNT_share
d.share.clust["BFS_Nr"] = d.share$BFS_Nr
d.share.clust["municipality"] = d.share$municipality
d.share.clust["canton"] = d.share$canton
d.share.clust["language"] = d.share$language
### 1. look at values for influence factors per cluster for k=6
### 2. print out values in a table for tableau
### 3. write text new above!
### 4. Same approach for cluster analysis canton
write.csv(d.share.clust, file="influence_factors_with_CA.csv")
par(mfrow=c(2,2))
boxplot(d.share.clust$GA_share ~ d.share.clust$k_cluster, col="blue",
main = "GA share for k-means clusters", xlab="cluster (nr)", ylab="share")
boxplot(d.share.clust$HTA_share ~ d.share.clust$k_cluster, col="green",
main = "HTA share for k-means clusters", xlab="cluster (nr)", ylab="share")
boxplot(d.share.clust$FNT_share ~ d.share.clust$k_cluster, col="purple",
main = "FNT share for k-means clusters", xlab="cluster (nr)", ylab="share")
# Loading data
d.cant.share <- read.csv("../Data/Cleaned/inf_fac_cant_share.csv")
# print(colnames(d.cant.share))
# select columns to normalize
columns <- c("PT_dist_medium", "PT_time_medium", "PT_dist_big", "PT_time_big",
"str_dist_medium", "str_time_medium" ,"str_dist_big", "str_time_big",
"PT_fact_big", "PT_fact_medium", "single_share", "married_share",
"widowed_share", "divorced_share", "GA_share", "HTA_share",
"FNT_share", "age0_20_share", "age20_40_share", "age40_60_share",
"age60._share", "birth_munic_share", "birth_cant_share",
"birth_CH_share", "birth_notCH_share", "male_share", "female_share",
"resid_0_1y_share", "resid_1_5y_share", "resid_6_10y_share",
"resid_10.y_share", "hh_1_share", "hh_2_share", "hh_3_5_share",
"hh_6._share", "bus_stops_per_pop", "other_stops_per_pop",
"train_stops_per_pop", "bus_stat_per_1000", "other_stat_per_1000",
"train_stat_per_1000", "comb_car_per_1000", "el_car_per_1000")
# normalize data
d.cant.norm.share <- d.cant.share
d.cant.norm.share[columns] <- scale(d.cant.norm.share[columns])
cl.an.cant.input <- d.cant.norm.share[,c("age0_20_share", "age20_40_share",
"birth_munic_share", "birth_cant_share", "birth_CH_share",
"male_share", "resid_6_10y_share", "hh_1_share", "hh_2_share",
"hh_3_5_share", "PT_time_medium", "PT_time_big", "train_stops_per_pop",
"bus_stat_per_1000", "other_stat_per_1000", "comb_car_per_1000",
"el_car_per_1000", "PT_fact_big", "single_share", "married_share",
"other_stops_per_pop", "train_stat_per_1000", "PT_fact_medium",
"bus_stops_per_pop")]
# set row names according to canton labels
row.names(cl.an.cant.input) <- d.cant.norm.share$canton
# distance matrix
dp_cant <- dist(cl.an.cant.input)
# Principal components
PC.inf.fac.cant <- princomp(cl.an.cant.input) # without language
summary(PC.inf.fac.cant, loadings = FALSE)
## first 2 PC explain 46% of variance, 5 PC about 75% of variance
plot(PC.inf.fac.cant, main="Principal components: Explained variance per PC")
# write two first components out
pc.cant <- PC.inf.fac.cant$scores[,1:2]
## Agglomerative clustering with complete linkage
cl.cant.inf.fac <- hclust(dp_cant, method = "complete")
## Show dendrogram
plot(cl.cant.inf.fac)
abline(h=9, col="red", lty=2)
## Split into groups
k = 5 # split at height 9 => 5 groups
grps_cant <- cutree(cl.cant.inf.fac, k = k)
# plotting clustering result
plot(pc.cant, pch = grps_cant, col=grps_cant, lwd=2, type="n") # add clustering result
legend("topleft", legend = 1:k, pch = 1:k, col=1:k, bty="n")
text(pc.cant[,1], pc.cant[,2], labels=d.cant.norm.share$canton, col=grps_cant)
## Look at means of clusters / centroids
aggregate(cl.an.cant.input, by=list(cluster=grps_cant), mean)
#############
## k-means ##
#############
## Choose number of clusters k with scree plot
nr <- 12
wss <- rep(0, nr)
for (i in 1:nr) wss[i] <- sum(kmeans(cl.an.cant.input, centers = i, nstart = 20)$withinss)
plot(1:nr, wss, type = "b", xlab = "Number of groups", ylab = "Within groups sum of squares")
## good spot at 4, test all from 2 to 10
par(mfrow=c(1,2), mar=c(5,4,4,2), cex.main = 0.9, cex.axis=0.9)
for(k in 2:10) {
## k-means with 3-10 centers
ckm.cant <- kmeans(cl.an.cant.input, centers = k, nstart = 10)
grpsKM.cant <- ckm.cant$cluster
## Silhouette Plot
plot(silhouette(grpsKM.cant, dp_cant),
main=paste("Silhouette plot with", k, "clusters"))
## visualize in PC 1 & 2
plot(pc.cant, pch = grpsKM.cant, col=grpsKM.cant, lwd=2,
main=paste("k-means clustering with", k, "clusters"), type="n")
# legend("topleft", legend = 1:k, pch = 1:k, col=1:k, bty="n")
text(pc.cant[,1], pc.cant[,2], labels=d.cant.norm.share$canton, col=grpsKM.cant)
}
### 4 clusters have the highest average silhouette width!
# k-means cluster with k=4
ckm4.cant <- kmeans(cl.an.cant.input, centers = 4, nstart = 10)
grpsKM4.cant <- ckm4.cant$cluster
#############
## DBSCAN  ##
#############
kNNdistplot(cl.an.cant.input, k = 15) ## -> eps value = 7.9 (sharp increase)
abline(h=7.9, col = "red", lty=2)
# setting parameters according to minPts = k and eps = sharp increase
for(i in (1:7)) {
for(j in (1:4)) {
dbs.cant <- dbscan(cl.an.cant.input, eps = i, minPts = j)
print(paste("minPts:", j, "; eps: ", i, "; nr. of clusters: ",
max(dbs.cant$cluster))) # 1
}
}
# minPts=1 and eps=5 lead to 7 clusters, eps=6 to 3 clusters. Test these!
minPts=2
eps=5
dbs.cant <- dbscan(cl.an.cant.input, eps = eps, minPts = minPts)
max(dbs.cant$cluster) # 7, look at values
## plot on PC 1 & 2
plot(pc.cant, pch = dbs.cant$cluster+1, col = dbs.cant$cluster+1, type="n")
legend("topleft", legend = 0:max(dbs.cant$cluster), pch = 1:(max(dbs.cant$cluster)+1),
col = 1:(max(dbs.cant$cluster)+1))
text(pc.cant[,1], pc.cant[,2], labels=d.cant.norm.share$canton, col=dbs.cant$cluster+1)
table(dbs.cant$cluster)
## Gaussian Mixture Models
mc.cant <- Mclust(cl.an.cant.input)
table(mc.cant$classification)
par(mfrow=c(1,2), mar=c(2,2,1.5,1))
# K-means with 4 clusters
plot(pc.cant, pch = grpsKM4.cant, col=grpsKM4.cant, main="K-means (4 clusters)", type="n")
legend("topright", legend = 1:4, pch = 1:4, col=1:4, bty="n", cex=0.8)
text(pc.cant[,1], pc.cant[,2], labels=d.cant.norm.share$canton, col=grpsKM4.cant)
# Agglomerative clustering
plot(pc.cant, pch = grps_cant, col=grps_cant,
main="Agglomerative clustering (5 clusters)", type="n") # add clustering result
legend("topright", legend = 1:5, pch = 1:5, col=1:5, bty="n", cex=0.8)
text(pc.cant[,1], pc.cant[,2], labels=d.cant.norm.share$canton, col=grps_cant)
## Silhouette values ##
# Silhouette plot from package "cluster"
par(mfrow=c(1,2), mar=c(5,4,4,2))
plot(silhouette(grpsKM4.cant, dp_cant), main="k-means Clustering (k=4)")
plot(silhouette(grps_cant, dp_cant), main="Agglomerative clustering")
d.cant.share.clust <- d.cant.share
d.cant.share.clust["k_cluster"] = grpsKM4.cant
d.cant.share.clust["aggl_cluster"] = grps_cant
# categorical data
d.cant.share.clust$k_cluster<-as.factor(d.cant.share.clust$k_cluster)
d.cant.share.clust$aggl_cluster<-as.factor(d.cant.share.clust$aggl_cluster)
write.csv(d.cant.share.clust, file="cant_influence_factors_with_CA.csv")
par(mfrow=c(2,2))
boxplot(d.cant.share.clust$GA_share ~ d.cant.share.clust$aggl_cluster, col="blue",
main = "GA share for agglom. clusters")
boxplot(d.cant.share.clust$HTA_share ~ d.cant.share.clust$aggl_cluster, col="green",
main = "HTA share for agglom. clusters")
boxplot(d.cant.share.clust$FNT_share ~ d.cant.share.clust$aggl_cluster, col="purple",
main = "FNT share for agglom. clusters")
par(mfrow=c(2,2))
boxplot(d.cant.share.clust$GA_share ~ d.cant.share.clust$k_cluster, col="blue",
main = "GA share for k-means clusters")
boxplot(d.cant.share.clust$HTA_share ~ d.cant.share.clust$k_cluster, col="green",
main = "HTA share for k-means clusters")
boxplot(d.cant.share.clust$FNT_share ~ d.cant.share.clust$k_cluster, col="purple",
main = "FNT share for k-means clusters")
save.image("G:/My Drive/MasterThesis/Scripts/Influence_factors.RData")
